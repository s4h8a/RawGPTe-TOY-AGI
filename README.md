# RawGPTe: AGI from Scratch

[![Training Status](https://img.shields.io/badge/training-live-green)]()
[![Phases Complete](https://img.shields.io/badge/phases-14%2F15-blue)]()
[![Parameters](https://img.shields.io/badge/parameters-153M-orange)]()

> **Not a chatbot. Not an LLM. A world model that learns physics from experience.**

## ğŸš€ What is This?

I spent 6 months building AGI from scratch in my bedroom.

**No OpenAI. No Google. No pre-trained models.**

Just PyTorch and a dream.

Here's what 14 phases of building a World Model looks like.

### Current AI (LLMs) is just autocomplete.

It doesn't understand:
- Physics
- Causality  
- Time
- Space

I wanted to build something that actually **UNDERSTANDS** the world, not just predicts text.

So I started from first principles.

## ğŸ“ˆ Phase 15 (Current): MASSIVE Scale-Up

**Before**: 77K parameters
**Now**: 153M parameters (2,000x larger)

- 12 transformer layers
- 8 attention heads
- 1024 hidden dimensions

Training live for 1-2 weeks straight.

This is where emergent capabilities appear.

### Current Status: Training RIGHT NOW

ğŸ“Š **Epoch 1, Step 30,000+**
ğŸ“‰ **Loss**: -2.37 (improving)
âš¡ **Speed**: 0.7 steps/sec on RTX 3090
ğŸ’¾ **Auto-checkpointing** every 100 steps

- 300,000 training transitions
- 10 diverse environments

**It's learning as we speak.**

## âœ¨ What Makes This NOT Just Another Neural Network

âœ… **Triquetra architecture** (World + Agent + Critic)
âœ… **Uncertainty quantification** (knows what it doesn't know)
âœ… **Curiosity-driven exploration**
âœ… **Temporal abstraction** (learns skills, not just steps)
âœ… **Grounded language interface**

## ğŸ¯ What It Can Do NOW

â€¢ Predict physics 10+ steps ahead
â€¢ Generalize across different environments
â€¢ Learn symbols from raw data
â€¢ Plan using learned models
â€¢ Estimate its own uncertainty

## ğŸš€ What It Will Do After Scale

â€¢ Zero-shot transfer to new tasks
â€¢ Long-horizon planning
â€¢ Real-time reasoning
â€¢ Genuine understanding

## ğŸ—ï¸ Architecture Highlights

### The Triquetra Core

Three interconnected systems that mimic human cognition:

1. **World Model**: Predicts `State + Action â†’ Next State`
2. **Agent**: Plans using learned models
3. **Critic**: Evaluates uncertainty and curiosity
4. **Memory**: Episodic + world-state
5. **Language**: Grounded in physical experience

### Scaled Model (Phase 15)
- 12 Transformer blocks
- 8 attention heads
- 1024 hidden dimensions
- Uncertainty quantification
- Temporal abstraction

## ğŸ“Š Live Training Metrics

| Metric | Value |
|--------|-------|
| Parameters | 153,000,000 |
| Architecture | 12-layer Transformer |
| Training Data | 300,000 transitions |
| Current Loss | -2.37 â†“ |
| Speed | 0.7 steps/sec |
| Training Status | ğŸ”´ LIVE |

## ğŸ“ˆ 14 Complete Phases

1. âœ… World model foundation
2. âœ… Constraint learning
3. âœ… Latent state structuring
4. âœ… Planning loop
5. âœ… Triquetra architecture
6. âœ… State discipline
7. âœ… Complex dynamics
8. âœ… Temporal abstraction
9. âœ… Curiosity-driven learning
10. âœ… Memory systems
11. âœ… Grounded language
12. âœ… Symbol emergence
13. âœ… Metacognition
14. âœ… Multi-world generalization
15. ğŸ”„ **Scale-up (IN PROGRESS)**

## ğŸ¯ Why This Matters

| Aspect | LLMs (GPT-4) | RawGPTe |
|--------|--------------|---------|
| Understanding | Statistical patterns | Causal physics |
| Generalization | Prompt-dependent | Environment-agnostic |
| Efficiency | Trillion parameters | 153M parameters |
| Grounding | Text only | Physical simulation |
| Uncertainty | Overconfident | Calibrated |

## ğŸš¦ Roadmap to AGI

**Current**: AGI-adjacent foundation  
**Phase 16**: Visual perception  
**Phase 17**: Continuous control  
**Phase 18**: Lifelong learning  
**Phase 19**: Real embodiment  

**Estimate**: 2-5 years to AGI-adjacent system

## ğŸ› ï¸ Quick Start

```bash
git clone https://github.com/s4h8a/RawGPTe-TOY-AGI
cd RawGPTe-TOY-AGI
pip install -r requirements.txt

# Run training
bash start_continuous_training.sh

# Monitor progress
python3 monitor.py
```

## ğŸ“ Technical Details

### Why This Approach Works

Current Large Language Models (LLMs) like GPT-4 are fundamentally limited:
- They learn statistical correlations in text
- They lack causal understanding
- They cannot plan or reason about physics
- They require massive amounts of data (trillions of tokens)

RawGPTe takes a different approach:

**Physics First**: Train on simulated environments where the rules are clear  
**Learn Causality**: Understand how actions affect the world  
**Build Understanding**: Develop true comprehension, not pattern matching  
**Scale Efficiently**: Achieve more with 153M parameters than LLMs with trillions

### The Triquetra Design

```
State + Action â†’ World Model â†’ Predicted Next State
                      â†“
              Uncertainty Quantifier
                      â†“
            Is prediction uncertain?
                      â†“
        YES: Trigger Curiosity â†’ Exploration
        NO: Use for Planning â†’ Agent Actions
```

This loop creates genuine curiosity-driven learning.

## ğŸ”® Vision: Real AGI

The ultimate goal isn't a better chatbot. It's a system that:

1. **Understands physics** - Can reason about real-world interactions
2. **Plans ahead** - Solves multi-step problems
3. **Learns from experience** - Improves without retraining
4. **Communicates clearly** - Language grounded in real understanding
5. **Knows its limits** - Aware of uncertainty
6. **Remains safe** - Can be aligned with human values

## ğŸ“ Contact

- **Email**: shn.koshy@gmail.com
- **GitHub**: [@s4h8a](https://github.com/s4h8a)

## ğŸ“„ License

MIT License - See LICENSE file for details

---

**Status**: ğŸ”´ Training Live | Last Updated: 2026-02-09